{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T10:15:04.838533Z",
     "start_time": "2019-09-06T10:15:04.108683Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T05:02:50.410492Z",
     "start_time": "2019-08-29T05:02:48.896436Z"
    }
   },
   "outputs": [],
   "source": [
    "for year in ('2017','2018','2019'):\n",
    "    weather_df = pd.read_csv(f'./snow_data/{year}asos.txt')\n",
    "    weather_df.rename({'valid':'datetime'},axis=1,inplace=True)\n",
    "\n",
    "    # Let 'GW' denote Good Weather (aka normal weather)\n",
    "    weather_df['wxcodes'] = weather_df.wxcodes.replace(np.NaN, 'GW')\n",
    "    np.unique(weather_df.wxcodes)\n",
    "\n",
    "    def is_intense_snow(s):\n",
    "        if '+' in s or '-' in s:\n",
    "            if 'SN' in s:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def is_snow(s):\n",
    "        if '+' not in s or '-' not in s:    \n",
    "            if 'SN' in s:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    weather_df['heavy_snow'] = weather_df['wxcodes'].apply(lambda x: is_intense_snow(x))\n",
    "    weather_df['snow'] = weather_df['wxcodes'].apply(lambda x: is_snow(x))\n",
    "\n",
    "    weather_df['datetime'] = pd.to_datetime(weather_df.datetime).dt.round('H')\n",
    "\n",
    "    # Drop rows with missing temperatures\n",
    "    missing_weather = weather_df.loc[weather_df.tmpc.isna()].index\n",
    "    weather_df.drop(missing_weather, inplace=True)\n",
    "\n",
    "    weather_df = weather_df.groupby('datetime').agg({'tmpc': np.max, 'heavy_snow': np.sum, 'snow': np.sum}).reset_index()\n",
    "\n",
    "    weather_df.to_csv(f'./data/{year}weather.csv')\n",
    "    del weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather data:\n",
    "- Replaced missing values with `'GW'` as default\n",
    "- Only took into account `snow`, since rain was deemed not important from the first assignment (subset)\n",
    "- Converted to `datetime` format, and rounded to the closest hour to merge (minute accuracy is not important)\n",
    "- Groupby `datetime` since we needed to take the average reading of the 4 weather stations\n",
    "- We take the **max** temperature to denote the \"worst case\" temperature of the day\n",
    "- And the snow metrics are the total sum of warnings during that hour (more snow warnings = more snowfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T05:29:57.073625Z",
     "start_time": "2019-08-29T05:22:25.514446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 01\n",
      "Processing 02\n",
      "Processing 03\n",
      "Processing 04\n",
      "Processing 11\n",
      "Processing 12\n",
      "Processing 01\n",
      "Processing 02\n",
      "Processing 03\n",
      "Processing 04\n",
      "Processing 11\n",
      "Processing 12\n"
     ]
    }
   ],
   "source": [
    "for year in ('2017','2018'):\n",
    "    output = pd.DataFrame()\n",
    "    for i in ('01','02','03','04','11','12'):\n",
    "        print(f\"Processing {i}\")\n",
    "        sample_data = pd.read_csv(f'./snow_data/yellow_tripdata_{year}-{i}.csv')\n",
    "\n",
    "        # Ensure all values fall within the defined values in the Dictionary specified\n",
    "        sample_data['RatecodeID'] = sample_data['RatecodeID'].apply(lambda x: int(x) if x in range(1, 7) else np.NaN)\n",
    "        sample_data['payment_type'] = sample_data['payment_type'].apply(lambda x: int(x) if x in range(1, 7) else np.NaN)\n",
    "\n",
    "        # Convert to datetime\n",
    "        sample_data['PUdatetime'] = pd.to_datetime(sample_data['tpep_pickup_datetime'])\n",
    "        sample_data['DOdatetime'] = pd.to_datetime(sample_data['tpep_dropoff_datetime'])\n",
    "\n",
    "        # Add trip duration by minutes\n",
    "        sample_data['trip_duration'] = sample_data['DOdatetime'] - sample_data['PUdatetime']\n",
    "        sample_data['trip_duration'] = sample_data['trip_duration'].dt.seconds // 60 # round to the nearest minute so we preserve int\n",
    "\n",
    "        sample_data.drop(['tpep_pickup_datetime','tpep_dropoff_datetime'],axis=1,inplace=True)\n",
    "\n",
    "        # This time, we exclude all non-positive trips (loss of money) since we are working with profitability, and\n",
    "        # the non-positive trips do not follow any known distribution\n",
    "        invalid_vals = sample_data[(sample_data[['trip_duration',\n",
    "                                                 'trip_distance','fare_amount','total_amount',\n",
    "                                                'extra']] <= 0).any(axis=1)].index\n",
    "\n",
    "        invalid_val_distribution = sample_data.loc[invalid_vals]\n",
    "\n",
    "        sample_data = sample_data.drop(axis=0, index=invalid_vals)\n",
    "        sample_data = sample_data.dropna()\n",
    "\n",
    "\n",
    "        output = pd.concat([output, sample_data])\n",
    "\n",
    "        del sample_data\n",
    "\n",
    "    weather = pd.read_csv(f'./data/{year}weather.csv', index_col=0)\n",
    "\n",
    "    weather['datetime'] = pd.to_datetime(weather['datetime'])\n",
    "\n",
    "    output['dow'] = output['PUdatetime'].dt.dayofweek\n",
    "    output['hour'] = output['PUdatetime'].dt.hour\n",
    "    output['datetime'] = output['PUdatetime'].dt.round('H')\n",
    "    output.drop(['PUdatetime','DOdatetime','improvement_surcharge'], axis=1, inplace=True)\n",
    "\n",
    "    output = pd.merge(output, weather, on='datetime', how='left').drop('datetime', axis=1)\n",
    "\n",
    "    output.reset_index().to_feather(f'./data/{year}taxi_weather.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taxi Data:\n",
    "- Firstly, remove all outright invalid instances according to the data dictionary\n",
    "- Convert to datetime\n",
    "- Add duration in minutes (Refer to ETL of first assignment)\n",
    "- **But**, we now remove all negative values / disputed trips. This is because we want to work with the profitability of each trip, not the *loss* of profitability \n",
    "    - i.e We don't want to be predicting the probability of a trip being disputed / no charge\n",
    "    - It means we can remove inconsistencies\n",
    "- We may consider a `datetime` model, but suggest that we use `dow` and `hour`\n",
    "- We take note that `improvement_surcharge` is a constant 30 cents, so we can remove the attribute with ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T10:18:02.981544Z",
     "start_time": "2019-09-06T10:16:01.904729Z"
    }
   },
   "outputs": [],
   "source": [
    "year = \"2019\"\n",
    "output = pd.DataFrame()\n",
    "for i in ('01','02','03','04'):\n",
    "    print(f\"Processing {i}\")\n",
    "    sample_data = pd.read_csv(f'./snow_data/yellow_tripdata_{year}-{i}.csv')\n",
    "\n",
    "    # Ensure all values fall within the defined values in the Dictionary specified\n",
    "    sample_data['RatecodeID'] = sample_data['RatecodeID'].apply(lambda x: int(x) if x in range(1, 7) else np.NaN)\n",
    "    sample_data['payment_type'] = sample_data['payment_type'].apply(lambda x: int(x) if x in range(1, 7) else np.NaN)\n",
    "\n",
    "    # Convert to datetime\n",
    "    sample_data['PUdatetime'] = pd.to_datetime(sample_data['tpep_pickup_datetime'])\n",
    "    sample_data['DOdatetime'] = pd.to_datetime(sample_data['tpep_dropoff_datetime'])\n",
    "\n",
    "    # Add trip duration by minutes\n",
    "    sample_data['trip_duration'] = sample_data['DOdatetime'] - sample_data['PUdatetime']\n",
    "    sample_data['trip_duration'] = sample_data['trip_duration'].dt.seconds // 60 # round to the nearest minute so we preserve int\n",
    "\n",
    "    sample_data.drop(['tpep_pickup_datetime','tpep_dropoff_datetime'],axis=1,inplace=True)\n",
    "\n",
    "    # This time, we exclude all non-positive trips (loss of money) since we are working with profitability, and\n",
    "    # the non-positive trips do not follow any known distribution\n",
    "    invalid_vals = sample_data[(sample_data[['trip_duration',\n",
    "                                             'trip_distance','fare_amount','total_amount',\n",
    "                                            'extra']] <= 0).any(axis=1)].index\n",
    "\n",
    "    invalid_val_distribution = sample_data.loc[invalid_vals]\n",
    "\n",
    "    sample_data = sample_data.drop(axis=0, index=invalid_vals)\n",
    "    sample_data = sample_data.dropna()\n",
    "\n",
    "\n",
    "    output = pd.concat([output, sample_data])\n",
    "\n",
    "    del sample_data\n",
    "\n",
    "weather = pd.read_csv(f'./data/{year}weather.csv', index_col=0)\n",
    "\n",
    "weather['datetime'] = pd.to_datetime(weather['datetime'])\n",
    "\n",
    "output['dow'] = output['PUdatetime'].dt.dayofweek\n",
    "output['hour'] = output['PUdatetime'].dt.hour\n",
    "output['datetime'] = output['PUdatetime'].dt.round('H')\n",
    "output.drop(['PUdatetime','DOdatetime','improvement_surcharge'], axis=1, inplace=True)\n",
    "\n",
    "output = pd.merge(output, weather, on='datetime', how='left').drop('datetime', axis=1)\n",
    "\n",
    "to_drop = output.loc[output['RatecodeID'] == 5].index\n",
    "\n",
    "to_drop = to_drop.append(output.loc[output['RatecodeID'] == 6].index)\n",
    "to_drop = to_drop.append(output.loc[output['payment_type'] == 3].index)\n",
    "to_drop = to_drop.append(output.loc[output['payment_type'] == 4].index)\n",
    "to_drop = to_drop.append(output.loc[output['payment_type'] == 5].index)\n",
    "to_drop = to_drop.append(output.loc[output['payment_type'] == 6].index)\n",
    "to_drop = to_drop.append(output.loc[output['fare_amount'] >= 500].index)\n",
    "to_drop = to_drop.append(output.loc[output['tolls_amount'] > 374.94].index)\n",
    "to_drop = to_drop.append(output.loc[output['total_amount'] <= 2.5].index)    \n",
    "output.drop(index=to_drop, axis=1, inplace=True)\n",
    "\n",
    "output = output[[\"PULocationID\",\"hour\", \"dow\",\"tmpc\",\"heavy_snow\",\"snow\"]]\n",
    "\n",
    "output.reset_index().to_feather(f'./ml_data/{year}test.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
