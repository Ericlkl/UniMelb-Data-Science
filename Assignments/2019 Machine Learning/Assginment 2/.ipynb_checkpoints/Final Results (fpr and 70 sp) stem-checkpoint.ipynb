{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "ps = nltk.PorterStemmer()\n",
    "wordnet = nltk.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectFpr, SelectFdr, chi2, mutual_info_classif\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "category = ['Sydney', 'Melbourne', 'Brisbane', 'Perth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping raw tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train-raw.tsv', sep='\\t', names=['label', 'body_text']).iloc[1:].reset_index(drop=True)\n",
    "\n",
    "dev = pd.read_csv('dev-raw.tsv', sep='\\t').reset_index()\n",
    "dev.drop('level_0', axis=1, inplace=True)\n",
    "dev.columns = ['label', 'body_text']\n",
    "\n",
    "test = pd.read_csv('test-raw.tsv', sep='\\t').reset_index().rename({'level_0': 'Id'},axis=1).set_index('Id').drop('level_1',axis=1)\n",
    "test.columns = ['body_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_dev, y_dev = train['body_text'], train['label'], dev['body_text'], dev['label']\n",
    "# dev meta is used for our meta-classifier\n",
    "X_test = test['body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "- Based on the presence of a word (rather than frequency\n",
    "- `binary=True` flag so all non-zero counts are one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    return ''.join([i.casefold() for i in text if i not in punctuation])\n",
    "\n",
    "def remove_num(text):\n",
    "    return ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "def remove_unicode(text):\n",
    "    return ' '.join([i for i in text.split() if '\\\\' not in i])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([wordnet.lemmatize(i) for i in text.split() if i not in stopwords])\n",
    "\n",
    "def preprocess(text):\n",
    "    no_links = remove_links(text)  \n",
    "    no_unicode = remove_unicode(no_links)\n",
    "    no_punc = remove_punc(no_unicode)\n",
    "    no_num = remove_num(no_punc)\n",
    "    cleaned = remove_stopwords(no_num)\n",
    "    return tknzr.tokenize(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vectorizer = CountVectorizer(analyzer=preprocess, binary=True, min_df=2).fit(X_train)\n",
    "X_binary = binary_vectorizer.transform(X_train) # word in presence of label\n",
    "\n",
    "# should be a list of unique words in every train tweet \n",
    "features = binary_vectorizer.get_feature_names() \n",
    "\n",
    "# len(features) \n",
    "# run through says we have 105006 unique words (using preprocess function)\n",
    "# the preprocessed data they provided had 184674 unqiue words so we've simplified a lot more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "- Top n% features\n",
    "- Using frequency (`CountVectorizer`)\n",
    "- Using `tfidf`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "results = pd.read_excel('feature_selection.xlsx', index_col=0)\n",
    "results.plot(alpha=0.9)\n",
    "plt.title(\"CountVect vs TFIDF\")\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.xlabel(\"Top n% of chi-square features\")\n",
    "plt.xticks([5,10,20,50,80])\n",
    "plt.savefig('feature_selection.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes performs well with `tfidf`, but as expected does much better with frequencies (more consistent and higher performace).\n",
    "- SVM performs marginally better with `tfidf` and handles several features well (more consistent and higher performance).\n",
    "- Logistic seems to initially perform well, but since we use a `multinomial` distribution it does much better with frequencies (not as consistent but if were to take it, frequency is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best = SelectFpr(chi2, alpha=0.05).fit(X_binary, y_train)\n",
    "fpr_chi2 = [features[i] for i in k_best.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best = SelectPercentile(mutual_info_classif, percentile=70).fit(X_binary, y_train)\n",
    "sp_mi = [features[i] for i in k_best.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best_features = set(fpr_chi2+sp_mi)\n",
    "# output to text so we don't have to keep redoing this step...\n",
    "with open('set_best_features.txt', 'w') as f:\n",
    "    for item in k_best_features:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fpr_chi2), len(sp_mi), len(k_best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_BEST_FEATURES = list()\n",
    "with open('set_best_features.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        K_BEST_FEATURES.extend(line.strip().split('\\n'))\n",
    "\n",
    "def feature_select(text):\n",
    "    no_links = remove_links(text)  \n",
    "    no_unicode = remove_unicode(no_links)\n",
    "    no_punc = remove_punc(no_unicode)\n",
    "    no_num = remove_num(no_punc)\n",
    "    cleaned = remove_stopwords(no_num)\n",
    "    tokens = tknzr.tokenize(cleaned)\n",
    "    return [i for i in tokens if i in K_BEST_FEATURES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection (freq)\n",
    "- NB\n",
    "- LR\n",
    "\n",
    "Use the top 80% features (This is the freq variant of our data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=feature_select).fit(X_train)\n",
    "X_train_freq = vectorizer.transform(X_train)\n",
    "X_dev_freq = vectorizer.transform(X_dev) # We fit Dev to Train\n",
    "X_test_freq = vectorizer.transform(X_test) # We fit Test to Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection (`tfidf`)\n",
    "- SVM\n",
    "\n",
    "Use the top 80% features\n",
    "(This is the `tfidf` variant of our data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer() # transform frequency to tfidf\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_freq)\n",
    "X_dev_tfidf = tfidf.fit_transform(X_dev_freq)\n",
    "X_test_tfidf = tfidf.fit_transform(X_test_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our Classifiers\n",
    "- Including lemmatization of tweets\n",
    "- `y_pred` is our prediction on the real `test` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(clf, X_test, y_test):\n",
    "    # generates a report summary\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred,target_names=category))\n",
    "    print(f'Accuracy: {100*accuracy_score(y_pred, y_test):.2f}%')\n",
    "    df = pd.DataFrame(confusion_matrix(y_test, y_pred, labels=category), index=category, columns=category)\n",
    "    sns.heatmap(df, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    return y_pred\n",
    "\n",
    "def random_search_tune(X_train, y_train, estimator, parameters):\n",
    "    # randomised search for hyperparameters\n",
    "    rs = RandomizedSearchCV(estimator, parameters, n_iter=50, cv=5, random_state=0, n_jobs=-1)\n",
    "    rs_fit = rs.fit(X_train, y_train)\n",
    "    display(rs_fit.cv_results_)\n",
    "    return rs_fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "- Uses a frequency based feature space\n",
    "- Takes the top 80% features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Multinomial NB')\n",
    "clf1 = MultinomialNB().fit(X_train_freq, y_train)\n",
    "report(clf1, X_dev_freq, y_dev)\n",
    "to_output = clf1.predict(X_test_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(to_output, index=test.index,columns=['Class']).to_csv(\"fpr_sp_preds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (One Vs Rest)\n",
    "- Uses a `tfidf` feature space\n",
    "- Takes the top 80% features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('One vs Rest SVM') \n",
    "clf2 = LinearSVC(max_iter=10000, random_state=0, tol=1e-05).fit(X_train_tfidf, y_train)\n",
    "svm_pred = report(clf2, X_dev_tfidf, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Logistic Regression\n",
    "- Uses a frequency based feature space\n",
    "- Takes the top 80% features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression (Multi-Class)') \n",
    "clf3 = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000).fit(X_train_freq, y_train)\n",
    "report(clf3, X_dev_freq, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Ensemble Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_clf1 = RandomForestClassifier(n_estimators=100, min_samples_split=50, \n",
    "                              min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "stacked_clf2 = MultinomialNB()\n",
    "stacked_clf3 = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[stacked_clf1, stacked_clf2, stacked_clf3],\n",
    "                            meta_classifier=lr, use_probas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf.fit(X_train_tfidf, y_train.replace(category,[0,1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.Series(sclf.predict(X_dev_tfidf)).replace([0,1,2,3], category)\n",
    "print(classification_report(y_dev, preds,target_names=category))\n",
    "print(f'Accuracy: {100*accuracy_score(preds, y_dev):.2f}%')\n",
    "df = pd.DataFrame(confusion_matrix(y_dev, preds, labels=category), index=category, columns=category)\n",
    "sns.heatmap(df, annot=True, fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.Series(sclf.predict(X_test_tfidf)).replace([0,1,2,3], category),index=test.index).to_csv(\"fixed_80_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
